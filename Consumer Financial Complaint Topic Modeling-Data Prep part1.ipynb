{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINANCIAL COMPLAINT TOPIC MODELING - NMF\n",
    "\n",
    "### Albert Opoku - Senior Statistical Consultant at Allianca data Inc\n",
    "\n",
    "#### Contact me:\n",
    "- twitter [@opalbert](https://twitter.com/opalbert)\n",
    "- linkedIn [Albert Opoku](https://www.linkedin.com/in/albertopokupmachinelearning/)\n",
    "- email opalkabert@gmail.com\n",
    "- website [www.opokualbert.com](https://opokualbert.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASE:\n",
    "\n",
    "Consumers provide feedback on financial products or services and our task is to extract the hidden themes/topics and assign each of the feedback documents to one of these themes or topics.\n",
    "\n",
    "### Solution:\n",
    "\n",
    "Train a Natural Language Processing machine learning model to extract the topics from each of the open-ended complaint text document.\n",
    "\n",
    "### Data Source:\n",
    "\n",
    "The data is downloaded from kaggle via this url: [consumer complaint data](https://www.kaggle.com/cfpb/us-consumer-finance-complaints)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Topic Modeling** is an unsupervized machine learning technique to discover the hidden/latent thematic structure in a large corpus of text documents.\n",
    "\n",
    "\n",
    "[Latent Dirichlet allocation (LDA)](http://jmlr.org/papers/volume3/blei03a/blei03a.pdf) and [Non-Negative Matrix Fatactorization (NMF)](https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf) are the two most popular topic modeling techniques. LDA uses a probabilistic approach where as NMF uses matrix factorization approach.\n",
    "\n",
    "\n",
    "<img src=\"NMF_Equation.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"DTM_.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Doc_Topic_Terms.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.decomposition import NMF\n",
    "import numpy as np\n",
    "import pickle \n",
    "import warnings  \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk \n",
    "# from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "# from nltk.stem import PorterStemmer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv(\"consumer_complaints.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many rows have missing values for consumer_complaint_narrative column\n",
    "df_orig.consumer_complaint_narrative.isnull().values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only the relevant column for this project\n",
    "df_orig = df_orig.loc[:,['consumer_complaint_narrative']]\n",
    "# df_orig.to_csv('consumer_complaint_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude all rows with null consumer_complaint_narrative\n",
    "df_orig.dropna(inplace=True)\n",
    "df_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 1000)\n",
    "pd.options.display.max_rows = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a doc id for merging the results back to the original file \n",
    "df_orig.insert(0, 'Doc_Id', range(0, 0 + len(df_orig)))\n",
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for use later\n",
    "df_orig.to_pickle('df_orig.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_orig.loc[:,['consumer_complaint_narrative','Doc_Id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word count for each document\n",
    "df['word_count'] = df['consumer_complaint_narrative'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "df.word_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to get a sizable data to work with in this tutorial\n",
    "df1= df[(df['word_count']>=191)&(df['word_count']<=255)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Removing unwanted characters\n",
    "import re\n",
    "df1['consumer_complaint_narrative'] = df1['consumer_complaint_narrative'].str.replace('X', '')\n",
    "df1['consumer_complaint_narrative'] = df1['consumer_complaint_narrative'].str.replace('{', '')\n",
    "df1['consumer_complaint_narrative'] = df1['consumer_complaint_narrative'].str.replace('}', '')\n",
    "df1['consumer_complaint_narrative'] = df1['consumer_complaint_narrative'].str.replace('/', '')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 most frequent words\n",
    "freq = pd.Series(' '.join(df1['consumer_complaint_narrative']).split()).value_counts()[:20]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with only nouns. Use NLTK to get only nouns in the corpus\n",
    "def nouns(text):\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)]\n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['data_nouns'] = pd.DataFrame(df1.consumer_complaint_narrative.apply(nouns))\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further cleaning, removing stopwords, lemmatizing\n",
    "import re\n",
    "temp =[]\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for sentence in df1['data_nouns']:\n",
    "    sentence = sentence.lower()\n",
    "    cleaner = re.compile('<.*?>')\n",
    "    sentence = re.sub(cleaner, ' ', sentence)  # Remove html tags\n",
    "    sentence = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    sentence = re.sub(r'[.|,|)|(|\\|/]',r' ',sentence) # removing puntuations\n",
    "    \n",
    "    words = [lemmatizer.lemmatize(word) for word in sentence.split() if word not in my_stop_words] # removing stopwords and lemm\n",
    "    temp.append(words)\n",
    "    \n",
    "final_X = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = []\n",
    "for row in final_X:\n",
    "    sequ = ''\n",
    "    for word in row:\n",
    "        sequ = sequ + ' ' + word\n",
    "    sent.append(sequ)\n",
    "final_X = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_X[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unwanted characters, numbers and symbols \n",
    "df1['cleaned'] = final_X\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for use later\n",
    "df1.to_pickle('data_prep.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = pd.read_pickle(data_prep.pkl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
